---
title: "INSERT A GOOD TITLE HERE"
output:
  bookdown::html_document2: default
  bookdown::pdf_document2: default
---



hypotheses

H1 -dimensions on which things organize
  tensions:
    top down vs. bottom up
    regional vs. local
    economic infrastructure vs. environment
H2 -org type
H3 -psychological distance
H4 -involvement



**INTRODUCTION**  
- ecology of games  
- network management  
- case of sea level rise  

**BACKGROUND**  
- sea level rise as general problem  
- sea level rise in case of Bay Area  
- regional governance efforts  

**RATIONALE**  
- governance as a multi-actor situation without formal hierarchy, instead largely horizontal approaches to steering and decision-making (e..g, Klijn et a. 2010).  
- theory about network management (klijn/edelenbos, provan/kenis), EG framework (Lubell, Berardo), perhaps a little ACF  
- preferred management strategies and policy tools as a function of the portfolio of issues someone is concerned about  
- research questions:  


```{r startup_code, echo = F,message=F,results='hide',warning=F}
RERUN_MLTA = TRUE
DROP_BARRIERS = TRUE
seed = 24

packs =c('tidyverse','purrr','data.table','statnet','latentnet','bipartite','lvm4net','Ternary',
         'ggthemes','here','ggnetwork','gridExtra','ggrepel','corrplot','htmlTable','readxl','nFactors','ggrepel','plotly','ggalluvial')
need = packs[!packs %in% names(installed.packages()[,2])]
invisible(sapply(need,function(x) suppressMessages(install.packages(x,type= 'source'))))
invisible(sapply(packs,function(x) suppressMessages(library(x,character.only = T))))

old = theme_set(theme_bw())
orig = readxl::read_excel('input/SLRSurvey_Full.xlsx')
orig$Q4[is.na(orig$Q4)]<-'Other'
#recode anything with fewer than 10 respondents as other
# see what wed recode
#as.data.table(table(orig$Q4))[order(-N),][N<10,]
orig$Q4[orig$Q4 %in% as.data.table(table(orig$Q4))[order(-N),][N<10,]$V1] <- 'Other'
orig$Q4[grepl('Other',orig$Q4)]<-'Other'
incidence_dt = fread("CurrentFiles/Data/AdjMatrix_MinOtherRecode.csv")

incidence_mat = as.matrix(incidence_dt[,-c('ResponseId','DK')])
rownames(incidence_mat)<-incidence_dt$ResponseId
#drop isolates
incidence_mat = incidence_mat[rowSums(incidence_mat)!=0,]
incidence_mat = incidence_mat[,!grepl('Other',colnames(incidence_mat))]

#create network object
bip_net = as.network(incidence_mat,matrix.type = 'incidence',bipartite = T,directed = F,loops = F)
#code actor types
bip_net %v% 'Actor_Type' <- orig$Q4[match(network.vertex.names(bip_net),orig$ResponseId)]
#code concept types
concept_types = fread('input/Combined_VectorTypes_NoNewOther.csv')
bip_net %v% 'Concept_Type' <- concept_types$Type[match(network.vertex.names(bip_net), concept_types$Vector)]
set.vertex.attribute(bip_net,'Concept_Type',value = 'Person',v = which(is.na(bip_net %v% 'Concept_Type')))

bip_net %v% 'id' <- network.vertex.names(bip_net)
bip_net %v% 'id_concept' <- ifelse({bip_net %v% 'Concept_Type'} == 'Person',network.vertex.names(bip_net),bip_net %v% 'Concept_Type')

bip_net %v% 'b1_dummy_b2_names' <- ifelse({bip_net %v% 'Concept_Type'} == 'Person','Person',bip_net %v% 'vertex.names')

#DROP BARRIERS#
if(DROP_BARRIERS){
bip_net = get.inducedSubgraph(x = bip_net,v = which(bip_net %v% 'Concept_Type' != 'Barrier'))
}
#convert to incidence matrix
Y = as.sociomatrix(bip_net)
Y = Y[,!grepl('Other',colnames(Y))]
Y<-Y[,sort(colnames(Y))]


```

  
**METHODS AND MATERIALS**  

**Data**  
- survey design and implementation  
- response statistics (sample, response rate, etc.)  
- basic survey descriptives  
  - summary table of actor train responses (experience, org type, etc.)  
  - three panel plot bar plot of frequency of choices for each of concerns, barriers, policies  
Figure \@ref(fig:figure-choice-percentages) shows the percentage of respondents who selected each item.

```{r figure-choice-percentages, echo = F,message = F,warning = F,fig.cap = 'Frequency of response choices in sample'}

mY = melt(Y)
mY$cat = {bip_net %v% 'Concept_Type'}[match(mY$Var2,bip_net %v% 'vertex.names')]
mY<-data.table(mY)
mY = mY[,mean(value),by=.(Var2,cat)][order(-V1)]
mY$fact = fct_reorder(mY$Var2,mY$V1,mean)

#ideally this would be a fact_wrap call but getting factors to drop out takes some effort
plist = list(ggplot(mY[cat =='Concern',]) + geom_point(aes(x = fact,y = 100 * V1))+
               geom_text(aes(x = fact,y = 100 * V1 +5,label = fact),hjust = 0) + scale_y_continuous(limits = c(0,100)) +facet_wrap(~cat) + 
  theme_bw() + theme(axis.title = element_blank(),axis.text.y = element_blank(),axis.ticks.y= element_blank())+coord_flip(),
ggplot(mY[cat =='Policy',]) + geom_point(aes(x = fact,y = 100 * V1))+
               geom_text(aes(x = fact,y = 100 * V1 +5,label = fact),hjust = 0) + scale_y_continuous(limits = c(0,100)) +facet_wrap(~cat) + 
  theme_bw() + theme(axis.title = element_blank(),axis.text.y = element_blank(),axis.ticks.y= element_blank())+coord_flip(),
if(!DROP_BARRIERS){
ggplot(mY[cat =='Barrier',]) + geom_point(aes(x = fact,y = 100 * V1))+
              geom_text(aes(x = fact,y = 100 * V1 +5,label = fact), hjust = 0) + scale_y_continuous(limits = c(0,100)) +facet_wrap(~cat) + theme_bw() + theme(axis.title = element_blank(),axis.text.y = element_blank(),axis.ticks.y= element_blank())+coord_flip()}else{NULL})

if(DROP_BARRIERS){grid.arrange(plist[[1]],plist[[2]],bottom= 'Respondent selection #',ncol=2)}
if(!DROP_BARRIERS){grid.arrange(plist[[1]],plist[[2]],plist[[3]],bottom= 'Respondent selection #',ncol=3)}
```
  
  

**Model**

Survey respondents identified up to three (each) from a range of issues, barriers, and strategies. Thus, each respondent is linked to between 0 and 9 system components. We are interested both in: (1) how actors in the governance network assort into different coalitions or preference groups; and (2) how different issues, barriers, and strategies assort into different "policy portfolios". To analyze both simultaneously, we use a *mixture of latent trait analyzers* (MLTA) (Gollini 2021) model. The MLTA model works like a combination of *latent class analysis* (LCA) and *latent trait analysis* (LTA). LCA is a clustering approach that identifies unobserved, underlying groups of respondents based upon observed responses. LTA is essentially a factor analysis for binary (or categorical) data--the goal being to represent respondents' choices for the `r ncol(Y)` concept options on a reduced set of underlying dimensions. In other words, LCA groups respondents and LTA groups responses.

An MLTA model does both--clustering of respondents and factoring of responses--at the same time. Observations are assumed to come from groups of respondents. Response choices (policies/concerns/barriers) are then assumed to be dependent upon group membership *and* a group-specific D-dimensional continuous latent trait variable. In other words, response variables are modeled as conditional on latent classes and latent traits. 

Responses are represented as a binary incidence matrix X~~nm, were *N* is the number of respondents and *M* is the # of response items. The number of classes and groups are preset prior to model fitting. We thus test the fit of models for range of group numbers (1 to 5, where G = 1 implies no subgroups) and trait dimensions (0 to 4, wherein a 0 dimension MLTA is identical to an LCA). 

```{r mlta_analysis,echo = F,message=F,results='hide',warning=F,results = 'hide'}
#### note -- mlta takes vectors instead of single D and G values -- but pblapply is used to parallelize #####
###DO NOT RUN FULL VERSION EXCEPT ON A FANCY COMPUTER WHEN YOU HAVE A LOT OF TIME -- HIGH DIMENSION MODELS TAKE FOREVER.....
### CURRENTLY RERUN_MLTA is set to false, so this chunk loads an rds file from an old fit ###

max_dims = 2
opts = data.table(d = 1:max_dims)

### this part takes a while, so I commented out and upload an RDS at the end ###
if(RERUN_LTA){
require(doParallel)
cluster = makeCluster(4)
registerDoParallel(cluster)
clusterExport(cl = cluster,varlist = list('opts','Y'))
clusterEvalQ(cl = cluster,require(lvm4net))
#mlta_tests = foreach(i = 1:nrow(opts)) %dopar% {lta(X = Y, D = opts$D[i], G = opts$G[i],wfix = opts$fix[i],nstarts = 5,maxiter = 1e3)}
lta2 = lvm4net::lta(X = Y, D = 2,nstarts = 5,maxiter = 1e3)

saveRDS(lta_tests,'temp_objects/lta_results.rds')
}
```

**Model Fit**
```{r mlta_results,warning=F,echo = F,message = F}
mlta_temp = readRDS('temp_objects/mlta_results.rds')
mlta_results = data.table(opts,BIC = sapply(mlta_temp,function(x) x$BIC))
mlta_results$G_fix = paste0('G = ',mlta_results$G,' (fixed slope = ',mlta_results$fix,')')
mlta_results$BIC <- round(mlta_results$BIC)
mlta_cast = dcast(mlta_results[order(BIC)],G_fix ~ D,value.var = 'BIC')
```


```{r tables-mlta_gof,echo = F,message = F,warning = F}
htmlTable(mlta_cast[,-1],caption = 'BIC scores by MLTA specification',rnames = mlta_cast$G_fix,header = paste0('D = ',0:max_dims),
          tfoot = '*fixed slope refers to whether slope is constant across all groups or group-specific')
```

Table \@ref(tab:mlta_results) shows Bayesian information criterion (BIC) goodness-of-fit scores for MLTAs fit to different combinations of group and dimension numbers and with slopes either fixed or varied across all subgroups.^[Briefly, slope parameters reflect within-group (for fixed slope = FALSE) or overall (for fixed slope = TRUE) heterogeneity for a given response variable--large slope parameters reflect larger differences in the probability of response between group members. Slope parameters also reflect interdependence between responses--two positive slopes mean that two items have a simultaneous probability of selection greater than the group median more often than would be expected if the items are locally independent (Gollini and Murphy 2014).]. The simplest, most restrictive model--no subgroups, no underlying trait dimension--is the most parsimonious. However, given the prevalance of several common responses shown in figure \@ref(fig:figure_choice_percentages), it is unsurprising that the single-cluster model performs fairly well. These broadly shared concerns and preferences, such as recognition that sea level rise poses problems for transportation and stormwater and belief that a SLR plan is needed, are associated with a more diverse array of secondary concerns and policy responses that can be teased out with a multidimensional model. Moreover, overall goodness-of-fit scores are fairly similar across specifications, particularly for fixed slope models where slope parameters are assumed to be constant across groups. Because we are interested in underlying differences among network subgroups, we perform a further series of factor analysis and cluster analysis methods to identify suitable values of *D* (underlying trait dimensions) and *G* (number of subgroups) for subsequent analysis.

*Identifying underlying trait dimensions*

As described above, an LTA model is essentially a factor analysis--thus, we can use typical factor analysis tools for identifying an appropriate number of dimensions. Figure \@ref(fig:figure_factor_analysis) plots Eigen values by factor number along with a series of non-graphical tests meant to identify an optimal number of factors. These tests are not all in agreement, ranging from a recommendation of 2 factors (the statistical "elbow" of the curve) to 17 (the number of factors with an Eigenvalue greater than 1). For parsimony, we select the lowest recommended value, *D* = 2, and fit an MLTA model with two underlying trait dimensions.

```{r figure_factor_analysis, echo = F,message = F,warning = F}
ev <- eigen(cor(Y)) # get eigenvalues
ap <- nFactors::parallel(subject=nrow(Y),var=ncol(Y),
  rep=100,cent=.05)

nS <- nScree(x=ev$values, aparallel=ap$eigen$qevpea)
plotnScree(nS,xlab = 'Factors',main = 'Scree plot of factor Eigenvalues')
```



*Identifying subgroups*

We take a similar approach to identifying an appropriate number of groups. Figure \@ref(fig:figure_k_means) hierarchically presents a series of k-means clustering results fit to different values of *k*. The top level shows a single cluster model (i.e., no subgroups), and the very bottom layer shows groups for *k* = 10. The arrows in figure \@ref(fig:figure_k_means) show how respondents change groupings as the number of clusters changes. At high k-values, we observe that the clusters are unstable--respondents who were grouped into separate clusters at lower k-values are now mixed up into totally new groupings. In this regard, *k* = 3 appears to be a good cluster value--at *k* = 4, clusters become less stable, while *k* = 2 appears to mask a distinction between two underlying subgroups.

```{r figure_k_means, echo = F,message = F,warning = F}
set.seed(24)
library(clustree)

k_vals = 1:10
tmp <- NULL
for (k in k_vals){
  tmp[k] <- kmeans(Y, k, nstart = 30)
}
df <- data.frame(tmp)
# add a prefix to the column names
colnames(df) <- k_vals
colnames(df) <- paste0("k",colnames(df))
# get individual PCA
df.pca <- prcomp(df, center = TRUE, scale. = FALSE)
ind.coord <- df.pca$x
ind.coord <- ind.coord[,1:2]
df <- bind_cols(as.data.frame(df), as.data.frame(ind.coord))
clustree(df, prefix = "k")

```

```{r message=F,warning=F,echo = F}
index2 = which(opts$D==D&opts$G==G&opts$fix==T)
group_probs = mlta_temp[[index2]]$z
gp = data.table(group_probs)
```

Given the results presented above, we fit a final MLTA model with *G* = 3 and *D* = 2, keeping item-response slopes fixed across subgroups. Using this model, figure \@ref(fig:figure_group_probability_map) shows the predicted probability of group membership. The total probability must sum to one. Most respondents have a very high probability of being in a single group, with a just a limited amount showing a more ambiguous prediction. Taking the highest probability for each respondent (across all three groups), the median maximum probability is `r round(median(apply(gp,1,max)),2)`, and the minimum is `r round(min(apply(gp,1,max)),2)`.


```{r figure_group_probability_map, echo=F, message=FALSE, warning=FALSE}
TernaryPlot(alab = "p(Group 1) \u2192", blab = "p(Group 2) \u2192", clab = "\u2190 p(Group 3)",
            point = 'up',atip = 'G1', btip = 'G2',ctip = 'G3',
            lab.col = c('red', 'darkgreen', 'blue'), tip.col = c('red','darkgreen','blue'),
            lab.cex = 0.8, grid.minor.lines = 0,
            grid.lty = 'solid', col = rgb(0.9, 0.9, 0.9), grid.col = 'white', 
            axis.col = rgb(0.6, 0.6, 0.6), ticks.col = rgb(0.6, 0.6, 0.6),
            axis.rotate = FALSE,
            padding = 0.08)
AddToTernary(points, coordinates = gp, pch = 21, cex = 1)
```


Just as respondents are clustered within groups, concepts are linked to underlying trait dimensions. We can plot the strength of these linkages by plotting the slopes for the logistic response functions--these are interpreted similarly to loadings in a factor analysis, in that a slope near 1 or -1 indicates a strong relationship between the variable and the underlying trait dimension, and a loading near 0 indicates a weak relationship.
```{r figure_factor_loadings, warning = F,message = F,echo = F,fig.height=5,fig.width = 5}
mod = lta2

beta_dt = data.table(t(mod$w),item = colnames(Y))
beta_dt$group = concept_types$Type[match(beta_dt$item,concept_types$Vector)]
circles <- data.frame( x0 = 0, y0 = 0, r = 1)

#rotate factors for interpretation
library(psych)
beta_dt[,1:D] <- data.table(factor.rotate(f =as.matrix(beta_dt[,1:D]),plot = F,angle = 115))
library(ggforce)
ggplot() +
      geom_circle(data = circles,aes(x0 = x0,y0 = y0,r = r),lty = 2, col = 'grey50') +
  geom_vline(xintercept = 0,lty = 2,col = 'grey40') +
  geom_hline(yintercept = 0,lty = 2,col = 'grey40') + 
geom_text_repel(data = beta_dt[group!=if(DROP_BARRIERS){'Barrier'}else{'KEEP'},],aes(x = `Dim 1`,y = `Dim 2`,label = item,col = group,fill = group),
                size = 2.5,max.overlaps = 100,min.segment.length = 0.2,show.legend = F) +
    geom_point(data = beta_dt[group!=if(DROP_BARRIERS){'Barrier'}else{'KEEP'},],aes(x = `Dim 1`,y = `Dim 2`,label = item,col = group,fill = group),
               pch = 19,show.legend = T) +
    scale_fill_colorblind(name = 'Concept type') +
  scale_color_colorblind(name = 'Concept type') +
  ggtitle('Slope estimates ("loadings") for item-response functions on latent traits') +
  theme_bw() + 
  scale_x_continuous(name= 'Slope for dimension 1',limits = c(-1,1)) + 
  scale_y_continuous(name= 'Slope for dimension 2',limits = c(-1,1)) + 
  theme(axis.title = element_text(size = 10,inherit.blank = F,family = 'Times',,margin =c(1,1,1,1)),
        legend.position = c(0.9,0.2)) 
```



**RESULTS**  

**Correlation between external items and concept responses**  
See google doc with results from external items and group correlations here: 
https://docs.google.com/document/d/1uz421G2w8q_PJQNSWrB-HOETRNiQCUw-MLa6p3zfwaw/edit?usp=sharing 

Using Spearman's rho, Probability of Group 1 membership significantly correlated with more work tasks related to SLR including project management and outreach, being in a CBO or NGO org, and showing more concern for both short and long term SLR impacts. Also correlated with anticipating impacts of SLR happening sooner in time and negatively correlated with being in a water special district or local government org. 

Group 2 membership significantly correlated with higher level of involvement with SLR and executive job tasks. Also correlated with enviro and water special district orgs and negatively correlated with NGO, CBO, and state government organizations. This group is correlated with higher short term awareness of SLR impacts but lower long term concern and higher assessment of risk agreement regionally. 

Group 3 is significantly correlated with local government organizations but shows correlations with lower involvement levels, fewer SLR related work tasks (particularly project management and outreach), and fewer information sources used in SLR work. Negatively correlated with being in an enviro SD or NGO and correlated with lower short and long term SLR awareness, lower short and long term SLR concern, and later anticipated timing of SLR impacts. Also lower assessment of regional agreement on risks. 
```{r correlate_external_groups, echo=F,insert = F,warning = F,message = F}
#Create list of group probabilities for respondents------------------------
#looks like group_probs is the probabilities and Y is cooccurrence matrix with row names as respondent ID

group_prob <- as.data.frame(group_probs)
#create dataframe of Y (row names are respondent ID) and add in the group 1 and group 2 probabilities
RespGroups <- as.data.frame(Y)

RespGroups$P_G1 <- group_prob$V1
RespGroups$P_G2 <- group_prob$V2
RespGroups$P_G3 <- group_prob$V3

RespGroups$respondentID <- row.names(RespGroups)


#Match that back to survey results--------------------------------------
survey_orig <- as.data.frame(orig)
RespGroups <- as.data.frame(RespGroups)
survey_orig$respondentID <- survey_orig$ResponseId


merged_survey <- left_join(RespGroups, survey_orig, by='respondentID')

#Use merged_survey to find connections between groups and survey attributes--------------
#create additional variable assigning to G1 G2 or G3 based on which has higher probability

merged_survey$Group <- ifelse(merged_survey$P_G1>merged_survey$P_G2 & merged_survey$P_G1>merged_survey$P_G3, 1, 
                              ifelse(merged_survey$P_G2>merged_survey$P_G1 & merged_survey$P_G2>merged_survey$P_G3, 2,3))

#table(merged_survey$Group)

#Crosstabs of group assignments with other variables
#table(merged_survey$Group, merged_survey$Q1_Num)

#table(merged_survey$Group, merged_survey$Q2_Personal)
#table(merged_survey$Group, merged_survey$Q2_OneOrg)
#table(merged_survey$Group, merged_survey$Q2_MultiOrg)

#table(merged_survey$Group, merged_survey$Q4)

#table(merged_survey$Group, merged_survey$Q5)

#table(merged_survey$Group, merged_survey$Q8_Sum)

#table(merged_survey$Group, merged_survey$Q8_Exec)
#table(merged_survey$Group, merged_survey$Q8_Policy)
#table(merged_survey$Group, merged_survey$Q8_Planning)
#table(merged_survey$Group, merged_survey$Q8_Science)
#table(merged_survey$Group, merged_survey$Q8_Gov)
#table(merged_survey$Group, merged_survey$Q8_PM)
#table(merged_survey$Group, merged_survey$Q8_Advocacy)
#table(merged_survey$Group, merged_survey$Q8_Outreach)
#table(merged_survey$Group, merged_survey$Q8_Other)

#table(merged_survey$Group, merged_survey$Q32_Sum)

#table(merged_survey$Group, merged_survey$Q11_STAware)
#table(merged_survey$Group, merged_survey$Q11_LTAware)

#table(merged_survey$Group, merged_survey$Q12_STConcern)
#table(merged_survey$Group, merged_survey$Q12_LTConcern)

#table(merged_survey$Group, merged_survey$WhenSLR)

#table(merged_survey$Group, merged_survey$RiskAgree)
#table(merged_survey$Group, merged_survey$ActionAgree)

#Correlations between group probailities and other variables
library(devtools)

correlations <- subset(merged_survey, select= c('P_G1', 'P_G2', 'P_G3', 'Q1_Num', 'Q2_Personal', 'Q2_OneOrg', 'Q2_MultiOrg', 'Q8_Sum', 'Q8_Exec', 'Q8_Policy', 'Q8_Planning', 'Q8_Science', 'Q8_Gov', 'Q8_PM', 'Q8_Advocacy', 'Q8_Outreach', 'Q8_Other', 'Q32_Sum', 'Q4_Ag', 'Q4_CBO', 'Q4_Ed', 'Q4_Enviro', 'Q4_EnviroSD', 'Q4_Fed', 'Q4_LocalGov', 'Q4_Media', 'Q4_Multijuris', 'Q4_Multistake', 'Q4_NGO', 'Q4_Other', 'Q4_Political', 'Q4_RegGov', 'Q4_State', 'Q4_Trade', 'Q4_WaterSD', 'Q11_STAware', 'Q11_LTAware', 'Q12_STConcern', 'Q12_LTConcern', 'WhenSLR', 'RiskAgree', 'ActionAgree'))


correlations[is.na(correlations)]=0


correlationmatrix <- cor(correlations, method="kendall")
correlationmatrix_sp <- cor(correlations, method="spearman")


library("corrplot")
library("Hmisc")
#note that corrplot which I used for this only allows pearson and spearman- if we want kendall will need to do it another way
data.rcorr.pear <- rcorr(as.matrix(correlations), type="pearson")
data.rcorr.sp <- rcorr(as.matrix(correlations), type="spearman")

data.coeff.sp <- data.rcorr.sp$r
data.p.sp <- data.rcorr.sp$P

significantp <- data.p.sp[,1:3]
threecolumncorr <- data.coeff.sp[,1:3]

significantp <- as.data.frame(significantp)
threecolumncorr <- as.data.frame(threecolumncorr)

significantp$PG1_p <- significantp$P_G1
significantp$PG2_p <- significantp$P_G2
significantp$PG3_p <- significantp$P_G3

threecolumncorr$PG1_r <- threecolumncorr$P_G1
threecolumncorr$PG2_r <- threecolumncorr$P_G2
threecolumncorr$PG3_r <- threecolumncorr$P_G3


significantp <- significantp[,4:6]
threecolumncorr <- threecolumncorr[,4:6]

threecolumncorr$names <- rownames(threecolumncorr)

corrmatrix <- as.data.frame(threecolumncorr$names)
corrmatrix$PG1_r <- threecolumncorr$PG1_r
corrmatrix$PG1_p <- significantp$PG1_p
corrmatrix$PG2_r <- threecolumncorr$PG2_r
corrmatrix$PG2_p <- significantp$PG2_p
corrmatrix$PG3_r <- threecolumncorr$PG3_r
corrmatrix$PG3_p <- significantp$PG3_p

corrmatrix.sigG1 <- subset(corrmatrix, corrmatrix$PG1_p<0.05)
corrmatrix.sigG2 <- subset(corrmatrix, corrmatrix$PG2_p<0.05)
corrmatrix.sigG3 <- subset(corrmatrix, corrmatrix$PG3_p <0.05)

```


*respondents mapped on latent variables*
We can map each individual survey respondent on the (D = 2) latent variables. But, because the model probabilistically clusters respondents by group (G = 3), what is actually estimated is the posterior mean for each individual on each latent variable conditional on being in group G. So, I use the group assignment probabilities to select the 'best' posterior means for each respondent.

```{r,echo = F,message = F,warning = F}
g_index = apply(mod$z,1,which.max)
list_of_locs = lapply(seq_along(g_index),function(i) mod$mu[i,,g_index[i]])
locs_df = data.table(do.call(rbind,list_of_locs))
locs_df$group = g_index
ggplot(locs_df,aes(x = V1,y = V2,col = as.factor(group))) + 
  geom_point() +
  scale_y_continuous(name = 'Location on latent dimension 2') + 
  scale_x_continuous(name = 'Location on latent dimension 1') + 
  scale_color_brewer(type = 'qual',palette = 2,name = 'Predicted group') + 
  theme(legend.position = c(0.8,0.1)) + 
  ggtitle('Estimated location of respondents in latent space') + 
  NULL

```

Obviously, this is super messy. There is clearly variance in positioning within and between groups, but the groups themselves don't correspond directly to positions in the latent space. To some extent, that makes sense -- groups are based on responses, and latent dimensions are based on responses, but the groups and latent dimensions aren't connected at all so the "solution" to the latent dimension model and the "solution" to the respondent grouping model can organize the data differently.


HERE IS AN EXAMPLE OF HOW WE CAN TRY TO GET AT THIS VARIANCE -- IN THIS EXAMPLE, THIS PLOTS D1 AND D2 POSITION BY ORG. TYPE. I DON'T KNOW ENOUGH ABOUT THE OTHER SURVEY ITEMS, BUT KYRA YOU CAN USE THIS CODE (AND THE ABOVE PLOT) TO SEE HOW DIFFERENT ACTOR-LEVEL VARIABLES PLAY OUT
```{r,echo = F,warnings = F,message = F}
sub = data.table(orig[orig$ResponseId %in% rownames(Y),])
sub[,V1:=NULL]
locs_df2 = cbind(locs_df,sub)

ggplot(locs_df2,aes(y = Q4)) + 
  geom_boxplot(aes(x = V1,col = 'D1'),width = 0.4,position = position_nudge(y = -0.25),fill = NA)+
  geom_boxplot(aes(x = V2,col = 'D2'),width = 0.4,position = position_nudge(y = 0.25),fill = NA)+
  ggtitle('position on latent dimensions by org. type') + 
  scale_color_tableau(name = 'dimension') +
  scale_x_continuous(name = 'location')

```



```{r,echo = F,warnings = F,message = F}
org_count = locs_df2[,.N,by=.(group,Q4)]
tot_count = locs_df2[,.N,by=.(group)]
setnames(tot_count,'N','group_total')
org_count = merge(org_count,tot_count)
ggplot(data =org_count,
  aes(fill = Q4,x = group,y = N/group_total ))+
    geom_col(position = position_dodge()) +
  coord_flip()  +
 scale_y_continuous(labels = scales::percent,name = '% of group total')
```



**groups mapped to item-response loadings**

We can then evaluate the intercept terms in each logistic response function to identify which items load heavily by group. Ignoring the model slope parameters. Here, it's clear that some items are identified by both groups (e.g., stormwater, transportation) while others are more excluvisely related to one and not the other (e.g., concern about DACs). Some are rare in both, like "commercial" and "property value". This also begins to reveal why increased dimensionality doens't seem to help model fit very much, many items are about as likely to be selected by one group as the other. Heuristically, items below the dashed line are more strongly associated with Group 1, and above are more strongly associated with Group 2.

```{r, message=F,warning=F,echo = F}
beta_dt = data.table(t(mod$b),keep.rownames = T)
beta_dt$concept = colnames(Y)
beta_dt$type = concept_types$Type[match(beta_dt$concept,concept_types$Vector)]


# 
g_base = ggplot(beta_dt) + geom_abline(a =0, b = 1,lty = 2) +
  scale_x_continuous(limits = c(-4,4)) +
  scale_y_continuous(limits = c(-4,4))
g1_vs_g2 = g_base +
  geom_point(aes(x = `Group 1`,y = `Group 2`)) +
  geom_text_repel(max.overlaps = 15,aes(x = `Group 1`,y = `Group 2`,label = concept))
g2_vs_g3 = g_base +
  geom_point(aes(x = `Group 2`,y = `Group 3`)) +
  geom_text_repel(max.overlaps = 15,aes(x = `Group 2`,y = `Group 3`,label = concept))
g1_vs_g3 = g_base +
  geom_point(aes(x = `Group 1`,y = `Group 3`)) +
  geom_text_repel(max.overlaps = 15,aes(x = `Group 1`,y = `Group 3`,label = concept))
grid.arrange(g1_vs_g2,g2_vs_g3,g1_vs_g3,ncol = 2,
             top ='Intercept estimates for item-response by group (p = 1 / exp(-beta))')

```


```{r, warning = F,message= F,echo = F}
library(plotly)



fig <- plot_ly(beta_dt, x = ~ `Group 1`, y = ~ `Group 2`, z = ~`Group 3`,
               color = ~type,
               text = ~concept, hoverinfo = 'text')
fig <- fig %>% add_markers()
fig <- fig %>% layout(scene = list(
                      title= list(title = 'Group item-responese intercepts'),
                       xaxis = list(title = 'intercept, group 1'),
                     yaxis = list(title = 'intercept, group 2'),
                     zaxis = list(title = 'intercept, group 3')))
fig
```


```{r}
#keepConcepts = network.vertex.names(bip_net)[{bip_net %v% 'Concept_Type'} %in% c('Policy','Concern')]

coFreq_all = rbindlist(lapply(1:G,function(g) {
coFreq = data.table(reshape2::melt(crossprod(Y[g_index==g,])))
coFreq$Var1_Type = {bip_net %v% 'Concept_Type'}[match(coFreq$Var1,bip_net %v% 'vertex.names')]
coFreq$Var2_Type = {bip_net %v% 'Concept_Type'}[match(coFreq$Var2,bip_net %v% 'vertex.names')]
coFreq = coFreq[Var1_Type=='Concern'&Var2_Type=='Policy',]
coFreq$group = paste0('G',g)
coFreq}))

coFreq_all$Var1 <- as.character(coFreq_all$Var1 )
coFreq_all$Var2 <- as.character(coFreq_all$Var2 )
coFreq_all <- coFreq_all[order(Var1,Var2),][value>0,]

ggplot(coFreq_all[,sum(value),by=.(Var1,Var2,Var1_Type,Var2_Type)],
       aes(y = V1, axis1 = Var1, axis2 = Var2)) +
  geom_alluvium(width = 1/12,aes(alpha = V1),fill= 'grey20') + 
  geom_stratum(fill = "black", colour = 'grey',width = .1) + 
  ggtitle('Concern and policy co-occurence') + 
  geom_label(stat = "stratum", aes(label = after_stat(stratum)))+
  scale_x_discrete(limits = c("Concern", "Policy"), expand = c(.05, .05)) +
  guides(alpha = F)
  
```


THIS IS THE SAME PLOT BUT FACETED SO EACH GROUP IS SEPARATE

```{r,warning=F,message = F,echo = F,fig.height = 15}
ggplot(coFreq_all,
       aes(y = value, axis1 = Var1, axis2 = Var2)) +
  geom_alluvium(width = 1/12,aes(alpha = value),fill= 'grey20') + 
  facet_wrap(~group,ncol = 1,scale = 'free_y') +
  geom_stratum(fill = "black", colour = 'grey',width = .1) + 
  ggtitle('Concern and policy co-occurence') + 
  geom_label(stat = "stratum", aes(label = after_stat(stratum)))+
  scale_x_discrete(limits = c("Concern", "Policy"), expand = c(.05, .05)) +
  guides(alpha = F)
```

**more results**  
**more results**  


**DISCUSSION**


**CONCLUSION**






